# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/td3.yml
program: train_sb3_wandb_td3.py
method: grid
name: rel_ik_sb3_td3_ur5e_reach_0_05_pose_grid_search_normalize_obs
metric:
  goal: maximize
  name: rollout/ep_rew_mean

parameters:
  seed:
    values: [42, 24]

  num_envs:
    value: 2048

  device:
    value: "cuda:0"

  n_timesteps: # iteration * n_steps * nenvs: 400 * 64 * 8192 = 209715200
    value: 104857600

  policy:
    value: 'MlpPolicy'

  batch_size:
    value: 256

  gamma:
    value: 0.95

  learning_rate:
    value: 1e-4

  train_freq:
    value: 4

  gradient_steps:
    value: 4

  buffer_size: 
    value: 1000000

  policy_delay:
    value: 4

  learning_starts: 
    value: 1000

  tau: 
    value: 0.02

  target_policy_noise:
    value: 0.2

  target_noise_clip:
    value: 0.4

  action_noise:
    value: NormalActionNoise

  action_sigma:
    value: 0.01

  # replay_buffer_class: 
  #   value: "HerReplayBuffer"

  # replay_buffer_kwargs: 
  #   value: "dict(goal_selection_strategy='future', n_sampled_goal=4)"

  policy_kwargs:
    parameters:
      activation_fn: 
        value: nn.Tanh
      net_arch:
        value: [256, 128]


  normalize_input:
    values: [False, True]

  normalize_value:
    value: False

  clip_obs:
    value: 50.0


#######################
# Previous parameters #
#######################







# program: train_sb3_wandb_td3.py
# method: grid
# name: rel_ik_sb3_td3_ur5e_reach_0_05_pose
# metric:
#   goal: maximize
#   name: rollout/ep_rew_mean

# parameters:
#   seed:
#     value: 42

#   device:
#     value: "cuda:0"

#   n_timesteps: # iteration * n_steps * nenvs: 400 * 64 * 8192 = 209715200
#     value: 209715200

#   policy:
#     value: 'MlpPolicy'

#   batch_size:
#     values: [256, 8192] 

#   gamma:
#     value: 0.99

#   learning_rate:
#     values: [1e-4, 3e-4]

#   train_freq:
#     value: 4

#   gradient_steps:
#     value: 4

#   buffer_size: 
#     value: 1000000

#   policy_delay:
#     value: 2

#   learning_starts: 
#     value: 10000

#   tau: 
#     value: 0.02

#   target_policy_noise:
#     value: 0.2

#   target_noise_clip:
#     value: 0.5

#   # action_noise:
#   #   values: [NormalActionNoise, OrnsteinUhlenbeckActionNoise]

#   # optimize_memory_usage:
#   #   value: True

#   policy_kwargs:
#     parameters:
#       activation_fn: 
#         value: nn.ELU
#       net_arch:
#         values: [[300, 300], [256, 128], [128, 64]]

#   normalize_input:
#     value: False

#   normalize_value:
#     value: False

#   clip_obs:
#     value: 50.0