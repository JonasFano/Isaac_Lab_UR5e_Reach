# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/sac.yml
program: train_sb3_wandb_sac.py
method: grid
name: rel_ik_sb3_sac_ur5e_reach_0_05_pose_4
metric:
  goal: maximize
  name: rollout/ep_rew_mean

parameters:
  seed:
    value: 42

  n_timesteps: # n_timesteps = total gradient updates * train_freq/gradient_steps = 52428800
    value: 209715200

  policy:
    value: 'MlpPolicy'

  batch_size:
    value: 16384

  gamma:
    value: 0.95

  ent_coef: 
    value: 0.01

  learning_rate:
    values: [5e-4, 1e-3]

  train_freq:
    value: 4

  gradient_steps:
    value: 4

  target_update_interval: 
    value: 1

  # target_entropy: 
  #   value: -6 # - action space dimensions (6 elements in TCP pose)
  
  buffer_size: 
    value: 1000000

  learning_starts: 
    value: 1000

  replay_buffer_class: 
    value: "HerReplayBuffer"

  replay_buffer_kwargs: 
    value: "dict(goal_selection_strategy='future', n_sampled_goal=4)"

  tau: 
    value: 0.02

  use_sde:
    values: [True, False]

  policy_kwargs:
    parameters:
      activation_fn: 
        values: ['nn.ELU', 'nn.ReLU', 'nn.Tanh']
      net_arch:
        value: [128, 64]


  normalize_input:
    value: False

  normalize_value:
    value: False

  clip_obs:
    value: 50.0